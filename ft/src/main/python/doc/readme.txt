工作总结：
	此次抓取工作，遇到一些问题，在这里总结如下：
	1.选择好工具
		1.1 python 编辑器（解决你的编码问题，并能debug，从而提高你的工作效率）
			开始选择Notepad作为python代码编辑器，出现的问题就是编码问题，不同的Python文件出现不同的编码，import在一起就彻底乱了；
			开始没有注意到这个问题，代码中有大量处理编码的代码，最终也没有解决所有中文编码的问题；但是代码在mac上使用vi查看又是乱码;
			所以请选择合适的IDE,我最终选择pycharm，才最终解决乱码问题；
			另外，IDE还有debug功能，这个很让我意外，python其实是个脚本语言，在我印象中，这类语言要debug非常难，但是有了ide就好办了！
		1.2 python及其三方包的版本（经验问题）
			选择什么样的版本，取决于你要是使用到的第三包的支持，这个很多程度上依赖你的经验
			开始安装使用3.3的最新版本，使用后发现有些三方包无法安装，听取三喜的建议说2.5版本很稳定，然后又安装了2.5版本，
			但是在安装pycurl时，发现2.5不支持pycurl，从2.6才开始支持，就又不得不又安装了2.6版本，
			最后发现，python的每个版本，其三方包也都有对应的版本，而且是不同OS,不同bits的，这个一定要对应上，否则出问题你很难发现
		1.3 html解析器
			html解析器有很多开源实现，我最终选择pyquery是因为我学习过jquery，当然现在也发现一些问题，有的html文本无法解析，最后还是自己直接解析文本来解决
	2. 网页的抓取
		首先面临的是网页的下载问题，开始出现很多的页面解析失败，以为是解析的问题，后来发现下载下来的页面很小，而且乱码，这个很难理解，因为页面本身是正常的，
		只是下载下来是乱码，也不完整，成功解析率只有30%，后来发现有的页面在第二次抓取解析的时候又成功了，
		所以怀疑是网络丢包问题（页面body信息收到，但是头部包丢失，无法获取指定编码），所以如果抓取失败的页面，会被再次抓取，并解析，反复解析那些失败的页面，
		发现成功率已经达到了95%以上。